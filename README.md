# Data-Mining-Project
# Credit Card Default Prediction
## 1.Business Understanding
**Project Objectives:**
The **primary objective** of this project is to develop a machine-learning model that accurately predicts credit card default. The model utilizes the K-Nearest Neighbors (KNN) algorithm to analyze customer data and determine the likelihood of default.
**Background:**
Predicting credit card default is important for financial institutions to mitigate risk (Bhandary & Ghosh, 2025). This tool addresses the **business need** to identify customers likely to default, enabling proactive measures. **Stakeholders**, including banks and credit card companies, are concerned with minimizing losses and maintaining healthy portfolios. They require tools that provide actionable insights and enable timely interventions. **Considerations** include potential biases in the dataset, model interpretability, and the need for real-time predictions. Acknowledging the inherent limitations of predictive models, the project recognizes the possibility of misclassifications.
**Success Criteria:**
1. Achieve an accuracy exceeding 80% on the test dataset.
2. Provide reliable predictions of default probability.
3. Ensure a user-friendly interface for inputting customer data and receiving predictions.
## 2. Data Understanding
**Data Collection:** The dataset, "UCI_Credit_Card (1).csv," contains credit card customer information and default status. It serves as the foundation for the prediction model.
**Data Description:** The dataset includes features such as credit limit, age, payment history, and bill amounts serving as the independent variables. The dependent variable is "default.payment.next.month," indicating whether a customer defaulted.
**Initial Exploration:** The dataset was analyzed to understand feature distributions and class balance. There is an imbalance in the dataset, with more non-default cases. Potential biases from data sources were noted.
## 3. Data Preparation
To **prepare** the data for the K-Nearest Neighbors (KNN) model, several essential steps were taken. First, feature selection was performed by removing the "ID" column, which was deemed irrelevant for prediction, and identifying the most relevant features. Subsequently, the dataset was split into 80% training data and 20% testing data, allowing for strong model training and evaluation. Finally, the features were **standardized** using StandardScaler to ensure optimal model performance. These preprocessing steps collectively ensured that the data was appropriately **formatted and scaled**, making it suitable for effective training of the KNN model.
## 4. Modeling
The KNN algorithm was **selected** for its effectiveness in classification tasks. The model was trained using **optimized hyperparameters** (n_neighbors=11, metric='manhattan') to increase accuracy. The previous model was using default parameters which led to 79% accuracy. After the adjustment the model is at 81% accuracy. The data from the model was **split** into training (80%) and test data(20%). The training data was used to **fit the model**, and predictions were made on the test set.
## 5.Evaluation
The classification report indicates a model with a reasonable overall performance, achieving an **accuracy of 81.15%**. While not as high, this accuracy still signifies a degree of correctness in its predictions. However, there are notable differences in performance between the two classes. Specifically, the model exhibits a precision of 83.79% for class 0 (non-defaults), meaning that 83.79% of instances predicted as non-defaults were actually non-defaults. For class 1 (defaults), the precision is 62.33%, indicating that the model is less precise in identifying actual defaults. Therefore, the model is more biased toward non-defaults. Furthermore, the model demonstrates a high **recall** of 94.07% for class 0, successfully identifying a large proportion of all non-default cases. However, the recall for class 1 is significantly lower at 35.03%, suggesting that unfortunately the model misses a substantial number of actual defaults.
The **F1-scores**, which balance precision and recall, reflect this disparity. For class 0, the F1-score is 88.63%, indicating a good balance. For class 1, the F1-score is 44.86%, revealing a less effective balance. The macro average F1-score is 66.74%, and the weighted average F1-score is 79.05%, further confirming the model's varying performance across the classes. The **support values** indicate that class 0 has 4687 instances and class 1 has 1313 instances, demonstrating a class imbalance with a higher representation of non-default cases.
The model implements a credit card default prediction model using the K-Nearest Neighbors (KNN) algorithm. Its **strengths** lie in its structured code, effective feature scaling, and a user-friendly Streamlit interface. The data splitting ensures fair testing. Additionally, the classification report offers detailed performance insights to the user. However, as a **weakness** the model demonstrates a **bias** towards predicting non-defaults, as evidenced by the significantly lower recall and F1-score for defaults.
## 6. Deployment
The model was **deployed** as a Streamlit web application, allowing users to input customer data and receive default predictions. Continuous **monitoring** of model performance and periodic retraining with new data will ensure accuracy. **Future work** includes exploring other algorithms, addressing class imbalance, and further tuning hyperparameters. **Challenges** include maintaining accuracy over time and handling potential biases.
## Conclusion
A credit card default prediction model using KNN was successfully developed. The model achieves a reasonable accuracy, with room for improvement in predicting defaults. Future efforts will focus on enhancing performance and addressing limitations and bias. As a first model this one personally taught us what models need to operate and how we can increase the performance of the model through different functions. Furthermore, we also learned how to deploy our models as an application.

